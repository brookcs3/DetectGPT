#!/usr/bin/env python3

"""
This version uses Groq-hosted LLaMA 3 models for fast perplexity estimation and AI detection.
Originally based on Hugging Face's perplexity example:
https://huggingface.co/docs/transformers/perplexity

Converted for Groq API integration by Cameron Brooks.
"""
import time
import itertools
import math
import numpy as np
import random
import re
from groq import Groq
import os
import requests

from collections import OrderedDict

from scipy.stats import norm
from difflib import SequenceMatcher
from multiprocessing.pool import ThreadPool

def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()

def normCdf(x):
    return norm.cdf(x)

def likelihoodRatio(x, y):
    return normCdf(x)/normCdf(y)

np.random.seed(0)

# Define the Groq model for deployment
GROQ_MODEL = "llama3-8b-8192"

class GroqLLaMA3:
    def __init__(self, device="cpu", model_id=GROQ_MODEL):
        self.device = device
        self.model_id = model_id
        self.client = Groq(api_key=os.environ.get("GROQ_API_KEY"))  # Initialize Groq client

        self.max_length = 8192  # Placeholder for max length
        self.stride = 51
        self.threshold = 0.7

    def generate_completion(self, messages):
        """
        Generate a completion using the Groq API.
        This method sends a request to the Groq API and retrieves the response.

        Args:
            messages (list): A list of dictionaries containing the role and content of the message.

        Returns:
            str: The content of the response message.
        """
        try:
            payload = {
                "model": self.model_id,
                "messages": messages
            }
            HEADERS = {
                "Authorization": f"Bearer {os.environ.get('GROQ_API_KEY', '')}",
                "Content-Type": "application/json"
            }
            print(f"Calling Groq API at https://api.groq.com/openai/v1/chat/completions with payload: {payload}")
            resp = requests.post(
                f"{os.environ.get('GROQ_API_BASE_URL', 'https://api.groq.com')}/openai/v1/chat/completions",
                headers=HEADERS,
                json=payload
            )
            resp.raise_for_status()  # Ensure HTTP errors are raised
            response = resp.json()
            # Validate response structure
            if not response or 'choices' not in response or not response['choices']:
                raise ValueError("Invalid response structure from Groq API")

            # Extract and return the content of the first choice
            return response['choices'][0]['message']['content']
        except Exception as e:
            # Log the error and return a meaningful message
            print(f"Error in generate_completion: {e}")
            return "Error: Unable to generate completion."

    def getPPL_1(self, sentence):
        messages = [
            {"role": "user", "content": sentence}
        ]
        max_retries = 3
        for attempt in range(max_retries):
            response = self.generate_completion(messages)
            if not isinstance(response, str):
                print(f"Invalid response received for perplexity calculation (Attempt {attempt + 1}): {response}")
                continue

            try:
                match = re.search(r"[-+]?\d*\.\d+|\d+", response)
                if match:
                    perplexity = float(match.group())  # Extract numeric value
                    if perplexity == 0:
                        print(f"Warning: Perplexity is zero for sentence: {sentence}")
                    print(f"Extracted perplexity: {perplexity} (Attempt {attempt + 1})")
                    return perplexity
                else:
                    print(f"No numeric value found in response (Attempt {attempt + 1}): {response}")
            except Exception as e:
                print(f"Error extracting perplexity (Attempt {attempt + 1}): {e}")

        # If all retries fail, return a fallback value
        print(f"Failed to calculate perplexity after {max_retries} attempts. Returning fallback value.")
        return float('inf')

    def getResults_1(self, threshold):
        if threshold < 60:
            label = 0
            return "The Text is generated by AI.", label
        elif threshold < 80:
            label = 0
            return "The Text is most probably contain parts which are generated by AI. (require more text for better Judgement)", label
        else:
            label = 1
            return "The Text is written by Human.", label

    def apply_extracted_fills(self, masked_texts, extracted_fills):
        texts = []
        for idx, (text, fills) in enumerate(zip(masked_texts, extracted_fills)):
            tokens = list(re.finditer(r"<extra_id_\d+>", text))
            if len(fills) < len(tokens):
                continue

            offset = 0
            for fill_idx in range(len(tokens)):
                start, end = tokens[fill_idx].span()
                text = text[:start+offset] + fills[fill_idx] + text[end+offset:]
                offset = offset - (end - start) + len(fills[fill_idx])
            texts.append(text)

        return texts

    # [DEPRECATED] Unmasking logic is currently not implemented for Groq and returns unchanged input.
    def unmasker(self, text, num_of_masks):
        # Placeholder for Groq-hosted unmasking logic
        # Replace T5-based logic with Groq API calls
        extracted_fills = []  # Placeholder for extracted fills from Groq API
        perturbed_texts = self.apply_extracted_fills(text, extracted_fills)

        return perturbed_texts


    def __call__(self, *args):
        version = args[-1]
        sentence = args[0]
        if version == "v1.1":
            return self.process_v1_1(sentence, args[1])
        elif version == "v1":
            return self.process_v1(sentence)
        else:
            return "Model version not defined"

    def process_v1(self, sentence):
        """
        Takes in a sentence split by full stop
        and calculates the perplexity of the total sentence.
        Splits the lines based on full stop and finds the perplexity of each sentence.
        """
        results = OrderedDict()

        total_valid_char = re.findall("[a-zA-Z0-9]+", sentence)
        total_valid_char = sum([len(x) for x in total_valid_char])  # Finds len of all the valid characters in a sentence

        lines = re.split(r'(?<=[.?!][ \[\(])|(?<=\n)\s*', sentence)
        lines = list(filter(lambda x: (x is not None) and (len(x) > 0), lines))

        perplexity = self.getPPL_1(sentence)
        print(f"Perplexity: {perplexity}")
        results["Perplexity"] = perplexity

        offset = ""
        perplexity_per_line = []
        sentence_lengths = []  # Track sentence lengths
        skipped_sentences = 0  # Count skipped sentences
        for i, line in enumerate(lines):
            if re.search("[a-zA-Z0-9]+", line.strip()) is None:
                skipped_sentences += 1
                continue
            if len(offset) > 0:
                line = offset + line
                offset = ""
            if line[0] == "\n" or line[0] == " ":
                line = line[1:]
            if line[-1] == "\n" or line[-1] == " ":
                line = line[:-1]
            elif line[-1] == "[" or line[-1] == "(":
                offset = line[-1]
                line = line[:-1]
            perplexity = self.getPPL_1(line)
            perplexity_per_line.append(perplexity)
            sentence_lengths.append(len(line.split()))  # Track sentence length in words

        # Calculate burstiness and average perplexity
        burstiness = max(perplexity_per_line) if perplexity_per_line else 0
        avg_perplexity = sum(perplexity_per_line) / len(perplexity_per_line) if perplexity_per_line else 0
        variance_perplexity = np.var(perplexity_per_line) if perplexity_per_line else 0
        median_perplexity = np.median(perplexity_per_line) if perplexity_per_line else 0

        print(f"Perplexity per line: {avg_perplexity}")
        results["Perplexity per line"] = avg_perplexity

        print(f"Burstiness: {burstiness}")
        results["Burstiness"] = burstiness

        print(f"Variance of perplexity: {variance_perplexity}")
        results["Variance of Perplexity"] = variance_perplexity

        print(f"Median perplexity: {median_perplexity}")
        results["Median Perplexity"] = median_perplexity

        print(f"Sentence lengths: {sentence_lengths}")
        results["Sentence Lengths"] = sentence_lengths

        print(f"Skipped sentences: {skipped_sentences}")
        results["Skipped Sentences"] = skipped_sentences

        out, label = self.getResults_1(results["Perplexity per line"])
        results["label"] = label

        return results, out

#################################ppp###############
#  Version 1.1 apis
###############################################

    def replaceMask(self, text, num_of_masks):
        list_generated_texts = self.unmasker(text, num_of_masks)

        return list_generated_texts

    def isSame(self, text1, text2):
        return text1 == text2

    # code took reference from https://github.com/eric-mitchell/detect-gpt
    def maskRandomWord(self, text, ratio):
        span = 2
        tokens = text.split(' ')
        mask_string = '<<<mask>>>'

        n_spans = ratio//(span + 2)

        n_masks = 0
        while n_masks < n_spans:
            start = np.random.randint(0, len(tokens) - span)
            end = start + span
            search_start = max(0, start - 1)
            search_end = min(len(tokens), end + 1)
            if mask_string not in tokens[search_start:search_end]:
                tokens[start:end] = [mask_string]
                n_masks += 1

        # replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments
        num_filled = 0
        for idx, token in enumerate(tokens):
            if token == mask_string:
                tokens[idx] = f'<extra_id_{num_filled}>'
                num_filled += 1
        assert num_filled == n_masks, f"num_filled {num_filled} != n_masks {n_masks}"
        text = ' '.join(tokens)
        return text, n_masks

    def multiMaskRandomWord(self, text, ratio, n):
        mask_texts = []
        list_num_of_masks = []
        for i in range(n):
            mask_text, num_of_masks = self.maskRandomWord(text, ratio)
            mask_texts.append(mask_text)
            list_num_of_masks.append(num_of_masks)
        return mask_texts, list_num_of_masks

    def getGeneratedTexts(self, args):
        original_text = args[0]
        n = args[1]
        texts = list(re.finditer(r"[^\d\W]+", original_text))
        ratio = int(0.3 * len(texts))

        mask_texts, list_num_of_masks = self.multiMaskRandomWord(original_text, ratio, n)
        list_generated_sentences = self.replaceMask(mask_texts, list_num_of_masks)
        return list_generated_sentences

    def mask(self, original_text, text, n=2, remaining=100):
        """
        text: string representing the sentence
        n: top n mask-filling to be choosen
        remaining: The remaining slots to be fill
        """

        if remaining <= 0:
            return []

        np.random.seed(0)
        start_time = time.time()
        out_sentences = []
        pool = ThreadPool(remaining//n)
        out_sentences = pool.map(self.getGeneratedTexts, [(original_text, n) for _ in range(remaining//n)])
        out_sentences = list(itertools.chain.from_iterable(out_sentences))
        end_time = time.time()

        return out_sentences

    def getVerdict(self, score):
        if score < self.threshold:
            return "This text is most likely written by an Human"
        else:
            return "This text is most likely generated by an A.I."

    def getScore(self, sentence):
        real_log_likelihood = self.getLogLikelihood(sentence)
        print(f"Real Log Likelihood: {real_log_likelihood}")  # Debug log

        # Generate multiple perturbed versions of the sentence
        perturbed_sentences = self.mask(sentence, sentence, n=2, remaining=10)
        generated_log_likelihoods = [self.getLogLikelihood(p) for p in perturbed_sentences]

        # Filter out invalid results
        generated_log_likelihoods = [x for x in generated_log_likelihoods if np.isfinite(x)]
        print(f"Generated Log Likelihoods: {generated_log_likelihoods}")  # Debug log

        if not generated_log_likelihoods:
            print("No valid generated log likelihoods found.")  # Debug log
            return -1, 0, 0

        mean_generated_log_likelihood = sum(generated_log_likelihoods) / len(generated_log_likelihoods)
        std_generated_log_likelihood = (sum((x - mean_generated_log_likelihood) ** 2 for x in generated_log_likelihoods) / len(generated_log_likelihoods)) ** 0.5

        diff = real_log_likelihood - mean_generated_log_likelihood
        score = diff / std_generated_log_likelihood if std_generated_log_likelihood != 0 else 0

        print(f"Score: {score}, Diff: {diff}, Std: {std_generated_log_likelihood}")  # Debug log
        return score, diff, std_generated_log_likelihood

    def getLogLikelihood(self, sentence):
        """
        Placeholder implementation for log likelihood calculation.
        In a real scenario, this should compute the log likelihood of the sentence using the model.
        Here, we use the negative perplexity as a proxy for demonstration.
        """
        perplexity = self.getPPL_1(sentence)
        if perplexity > 0 and np.isfinite(perplexity):
            return -math.log(perplexity)
        else:
            return float('-inf')

    def process_v1_1(self, sentence, chunk_value):
        sentence = re.sub(r"\\[[0-9]+\\]", "", sentence) # remove all the [numbers] cause of wiki

        words = re.split("[ \n]", sentence)

        groups = len(words) // chunk_value + 1
        lines = []
        stride = len(words) // groups + 1
        for i in range(0, len(words), stride):
            start_pos = i
            end_pos = min(i+stride, len(words))

            selected_text = " ".join(words[start_pos:end_pos])
            selected_text = selected_text.strip()
            if selected_text == "":
                continue

            lines.append(selected_text)

        # Initialize metadata dictionary
        metadata = {
            "sentence_lengths": [],
            "skipped_sentences": 0,
            "burstiness": 0,
            "processing_time": 0,
            "perplexity_per_line": [],
            "mean_perplexity": 0,
            "variance_perplexity": 0,
            "median_perplexity": 0
        }

        start_time = time.time()  # Start processing time
        # Add debug logs to verify the contents of perplexity_per_line and calculated statistics
        for i, line in enumerate(lines):
            if re.search("[a-zA-Z0-9]+", line) is None:
                print(f"Skipped line {i} due to lack of alphanumeric content: {line}")  # Debug log
                print(f"Skipped line {i}: {line}")  # Debug log
                continue

            perplexity = self.getPPL_1(line)
            print(f"Line {i} Perplexity: {perplexity}")  # Debug log
            metadata["perplexity_per_line"].append(perplexity)
            metadata["sentence_lengths"].append(len(line.split()))

        # Calculate statistics
        if metadata["perplexity_per_line"]:
            metadata["mean_perplexity"] = sum(metadata["perplexity_per_line"]) / len(metadata["perplexity_per_line"])
            metadata["variance_perplexity"] = np.var(metadata["perplexity_per_line"])
            metadata["median_perplexity"] = np.median(metadata["perplexity_per_line"])
            metadata["burstiness"] = max(metadata["perplexity_per_line"])
        else:
            metadata["mean_perplexity"] = 0
            metadata["variance_perplexity"] = 0
            metadata["median_perplexity"] = 0
            metadata["burstiness"] = 0

        metadata["processing_time"] = time.time() - start_time

        print(f"Perplexity per line: {metadata['mean_perplexity']}")
        print(f"Burstiness: {metadata['burstiness']}")
        print(f"Variance of perplexity: {metadata['variance_perplexity']}")
        print(f"Median perplexity: {metadata['median_perplexity']}")
        print(f"Sentence lengths: {metadata['sentence_lengths']}")
        print(f"Skipped sentences: {metadata['skipped_sentences']}")
        print(f"Processing time: {metadata['processing_time']} seconds")

        # Get AI detection score using the more sophisticated method
        score, diff, std = self.getScore(sentence)
        metadata["detection_score"] = score
        metadata["log_likelihood_diff"] = diff
        metadata["log_likelihood_std"] = std

        # Get a verdict based on both scoring methods
        score_verdict = self.getVerdict(score)
        perplexity_verdict, perplexity_label = self.getResults_1(metadata["mean_perplexity"])

        # Use the more confident determination (combining both methods)
        if score >= self.threshold or metadata["mean_perplexity"] < 60:
            label = 0
            out = f"AI-generated text detected. {score_verdict}"
        else:
            label = 1
            out = f"Human-written text detected. {score_verdict}"
        metadata["label"] = label
        return metadata, out

# Remove duplicate and mis-indented functions below, as their logic is already present in the class above.
